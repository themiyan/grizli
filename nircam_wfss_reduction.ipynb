{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Please configure your MAST API token and the directory where the data will be downloaded. You also need to specify the JWST program ID and observation number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mast API token\n",
    "# Replace 'YOUR_MAST_API_TOKEN' with your actual MAST API token\n",
    "MAST_API_TOKEN = 'YOUR_MAST_API_TOKEN'\n",
    "\n",
    "# Directory to download data\n",
    "# Replace '/path/to/your/download/directory' with your desired download path\n",
    "DOWNLOAD_DIR = '/path/to/your/download/directory'\n",
    "\n",
    "# JWST Program ID and Observation\n",
    "PROGRAM_ID = '6480'\n",
    "OBSERVATION = '015'\n",
    "\n",
    "# Print the configuration to confirm\n",
    "print(f\"MAST API Token: {MAST_API_TOKEN}\")\n",
    "print(f\"Download Directory: {DOWNLOAD_DIR}\")\n",
    "print(f\"Program ID: {PROGRAM_ID}\")\n",
    "print(f\"Observation: {OBSERVATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Download\n",
    "\n",
    "This section queries the MAST archive for JWST program 6480, observation 15, and downloads the relevant _rate.fits files for both direct imaging and WFSS exposures. These files are the input for Stage 1 of the JWST calibration pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# Login to MAST\n",
    "# Ensure MAST_API_TOKEN is defined in a previous cell or set as an environment variable\n",
    "if MAST_API_TOKEN != 'YOUR_MAST_API_TOKEN':\n",
    "    Observations.login(MAST_API_TOKEN)\n",
    "else:\n",
    "    print(\"Please set your MAST_API_TOKEN in the configuration cell.\")\n",
    "\n",
    "# Create download directory if it doesn't exist\n",
    "if not os.path.exists(DOWNLOAD_DIR):\n",
    "    os.makedirs(DOWNLOAD_DIR)\n",
    "    print(f\"Created directory: {DOWNLOAD_DIR}\")\n",
    "else:\n",
    "    print(f\"Directory {DOWNLOAD_DIR} already exists.\")\n",
    "\n",
    "# Search for observations\n",
    "# We are looking for level 1 (uncalibrated) science products for NIRCam\n",
    "# Suffixes for rate files can be 'rateints.fits' or 'rate.fits'\n",
    "obs_table = Observations.query_criteria(\n",
    "    obs_collection=\"JWST\",\n",
    "    instrument_name=\"NIRCAM/IMAGE\", # For direct imaging\n",
    "    proposal_id=PROGRAM_ID,\n",
    "    obsid=OBSERVATION,\n",
    "    dataproduct_type=\"image\", # Usually 'image' for direct imaging, 'spectrum' for WFSS, but let's be broad and filter later\n",
    "    calib_level=1 # Stage 0 or 1 products\n",
    ")\n",
    "\n",
    "# Filter for specific file types we need (rate files)\n",
    "# This is a basic filter; more specific filtering might be needed depending on exact file naming conventions\n",
    "product_list = Observations.get_product_urls(obs_table['obsid'])\n",
    "rate_files_to_download = [\n",
    "    url for url in product_list \n",
    "    if url.endswith('_rate.fits') or url.endswith('_rateints.fits')\n",
    "]\n",
    "\n",
    "print(f\"Found {len(rate_files_to_download)} rate files to download.\")\n",
    "\n",
    "# Download the files\n",
    "manifest = None # Initialize manifest\n",
    "if rate_files_to_download:\n",
    "    manifest = Observations.download_products(\n",
    "        rate_files_to_download,\n",
    "        download_dir=DOWNLOAD_DIR\n",
    "    )\n",
    "    print(f\"Download manifest: {manifest}\")\n",
    "else:\n",
    "    print(\"No rate files found to download with the current criteria.\")\n",
    "\n",
    "# Now search for WFSS data (typically has 'GRISM' in exposure type or similar)\n",
    "# The obsid might be different for WFSS, or it might be part of the same observation set.\n",
    "# We need to be careful with query criteria for WFSS.\n",
    "# For now, let's assume WFSS data might be identified by dataproduct_type='spectrum' or specific suffixes.\n",
    "\n",
    "obs_table_wfss = Observations.query_criteria(\n",
    "    obs_collection=\"JWST\",\n",
    "    instrument_name=\"NIRCAM/GRISM\", # For WFSS\n",
    "    proposal_id=PROGRAM_ID,\n",
    "    obsid=OBSERVATION,\n",
    "    dataproduct_type=\"spectrum\", # More likely for WFSS\n",
    "    calib_level=1\n",
    ")\n",
    "\n",
    "product_list_wfss = Observations.get_product_urls(obs_table_wfss['obsid'])\n",
    "wfss_rate_files_to_download = [\n",
    "    url for url in product_list_wfss\n",
    "    if url.endswith('_rate.fits') or url.endswith('_rateints.fits')\n",
    "]\n",
    "\n",
    "print(f\"Found {len(wfss_rate_files_to_download)} WFSS rate files to download.\")\n",
    "\n",
    "manifest_wfss = None # Initialize manifest_wfss\n",
    "if wfss_rate_files_to_download:\n",
    "    manifest_wfss = Observations.download_products(\n",
    "        wfss_rate_files_to_download,\n",
    "        download_dir=DOWNLOAD_DIR\n",
    "    )\n",
    "    print(f\"WFSS Download manifest: {manifest_wfss}\")\n",
    "else:\n",
    "    print(\"No WFSS rate files found to download with the current criteria for NIRCAM/GRISM.\")\n",
    "\n",
    "# It's possible direct image and grism exposures are mixed under NIRCAM/IMAGE or NIRCAM/GRISM\n",
    "# A more robust approach might be to download all level 1 products for the obsid and then filter locally,\n",
    "# or refine the query based on detailed knowledge of how products are tagged.\n",
    "# For now, we'll try to get both.\n",
    "\n",
    "print(\"\\n--- Data Download Summary ---\")\n",
    "downloaded_files = []\n",
    "if manifest and 'Local Path' in manifest.colnames:\n",
    "    for path in manifest['Local Path']:\n",
    "        downloaded_files.append(path)\n",
    "        print(f\"Downloaded: {os.path.basename(path)}\")\n",
    "if manifest_wfss and 'Local Path' in manifest_wfss.colnames:\n",
    "     for path in manifest_wfss['Local Path']:\n",
    "        downloaded_files.append(path)\n",
    "        print(f\"Downloaded: {os.path.basename(path)}\")\n",
    "\n",
    "if not downloaded_files:\n",
    "    print(\"No files were downloaded. Please check your MAST token, program ID, observation ID, and internet connection.\")\n",
    "    print(\"Also, verify the query criteria and product availability on MAST.\")\n",
    "else:\n",
    "    print(f\"Total files downloaded: {len(downloaded_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 1 Data Reduction (Detector1Pipeline)\n",
    "\n",
    "This section runs the JWST Stage 1 calibration pipeline (`Detector1Pipeline`) on the downloaded _rate.fits or _uncal.fits files. This pipeline performs basic detector-level corrections, such as ramp fitting, dark subtraction, and cosmic ray flagging, producing _cal.fits (or _calints.fits) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "\n",
    "# --- Stage 1 Processing ---\n",
    "# Identify all downloaded rate files (both direct image and WFSS)\n",
    "# We assume they are all in the DOWNLOAD_DIR/mastDownload/JWST/<obs_folder>\n",
    "# The exact subfolder structure might vary, so we search recursively.\n",
    "\n",
    "rate_files_pattern = os.path.join(DOWNLOAD_DIR, '**', '*_rate.fits')\n",
    "uncal_files_pattern = os.path.join(DOWNLOAD_DIR, '**', '*_uncal.fits') # Detector1Pipeline can also start from _uncal.fits\n",
    "\n",
    "# Glob for both patterns\n",
    "rate_files = glob.glob(rate_files_pattern, recursive=True)\n",
    "uncal_files = glob.glob(uncal_files_pattern, recursive=True)\n",
    "\n",
    "# Combine and unique the list of files to process\n",
    "files_to_process = list(set(rate_files + uncal_files))\n",
    "\n",
    "print(f\"Found {len(files_to_process)} rate/uncal files for Stage 1 processing:\")\n",
    "for f in files_to_process:\n",
    "    print(f\" - {os.path.basename(f)}\")\n",
    "\n",
    "# Output directory for Stage 1 products\n",
    "stage1_output_dir = os.path.join(DOWNLOAD_DIR, 'stage1_products')\n",
    "if not os.path.exists(stage1_output_dir):\n",
    "    os.makedirs(stage1_output_dir)\n",
    "    print(f\"Created directory: {stage1_output_dir}\")\n",
    "\n",
    "processed_cal_files = []\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"No rate or uncal files found to process. Please check the download step.\")\n",
    "else:\n",
    "    for input_file in files_to_process:\n",
    "        print(f\"\\nProcessing file: {os.path.basename(input_file)}\")\n",
    "        try:\n",
    "            # Initialize the pipeline\n",
    "            pipeline_stage1 = Detector1Pipeline()\n",
    "\n",
    "            # Set output directory for the products of this pipeline\n",
    "            pipeline_stage1.output_dir = stage1_output_dir\n",
    "            \n",
    "            # Optionally, save calibrated ramp (if starting from rateints)\n",
    "            # pipeline_stage1.save_calibrated_ramp = True \n",
    "\n",
    "            # Run the pipeline\n",
    "            pipeline_stage1.run(input_file)\n",
    "            \n",
    "            # Determine the expected output filename\n",
    "            # Input: jw<propid><obsid>_<...>_rate.fits or _uncal.fits\n",
    "            # Output: jw<propid><obsid>_<...>_cal.fits or _calints.fits\n",
    "            base_name = os.path.basename(input_file)\n",
    "            if base_name.endswith('_rate.fits'):\n",
    "                output_cal_file = base_name.replace('_rate.fits', '_cal.fits')\n",
    "            elif base_name.endswith('_uncal.fits'):\n",
    "                 output_cal_file = base_name.replace('_uncal.fits', '_cal.fits')\n",
    "            else: # Should not happen if filtering is correct\n",
    "                print(f\"Warning: Unexpected file suffix for {base_name}\")\n",
    "                continue\n",
    "\n",
    "            output_cal_path = os.path.join(stage1_output_dir, output_cal_file)\n",
    "            \n",
    "            if os.path.exists(output_cal_path):\n",
    "                print(f\"Successfully processed. Output: {output_cal_file}\")\n",
    "                processed_cal_files.append(output_cal_path)\n",
    "            else:\n",
    "                # Check for _calints.fits if _cal.fits is not found (e.g. if save_calibrated_ramp was True)\n",
    "                output_calints_file = output_cal_file.replace('_cal.fits', '_calints.fits')\n",
    "                output_calints_path = os.path.join(stage1_output_dir, output_calints_file)\n",
    "                if os.path.exists(output_calints_path):\n",
    "                     print(f\"Successfully processed. Output: {output_calints_file}\")\n",
    "                     processed_cal_files.append(output_calints_path)\n",
    "                else:\n",
    "                    print(f\"Error: Output file {output_cal_file} or {output_calints_file} not found after processing {input_file}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_file} with Detector1Pipeline: {e}\")\n",
    "\n",
    "print(\"\\n--- Stage 1 Processing Summary ---\")\n",
    "if processed_cal_files:\n",
    "    print(\"Successfully processed the following files to Stage 1 (_cal.fits or _calints.fits):\")\n",
    "    for f_cal in processed_cal_files:\n",
    "        print(f\" - {os.path.basename(f_cal)} in {stage1_output_dir}\")\n",
    "else:\n",
    "    print(\"No files were successfully processed by Stage 1 pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Alignment and Combination (Image2Pipeline & Image3Pipeline for Direct Images)\n",
    "\n",
    "This section processes the direct imaging _cal.fits files through the JWST Stage 2 (`Image2Pipeline`) and Stage 3 (`Image3Pipeline`) pipelines.\n",
    "- `Image2Pipeline` performs additional instrument corrections (e.g., flat fielding, WCS refinement) producing _i2d.fits files.\n",
    "- `Image3Pipeline` aligns and combines multiple _i2d.fits exposures (if available) into a single, deeper mosaic image (e.g., _drz.fits or _i2d.fits).\n",
    "If only one direct image exposure is present, Image3Pipeline might be skipped, and the output of Image2Pipeline will be used. The final combined/processed direct image will be used for creating segmentation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from jwst.pipeline import Image2Pipeline, Image3Pipeline\n",
    "from jwst.associations import asn_from_list\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "\n",
    "# --- Stage 2 and 3 Processing for Direct Images ---\n",
    "# First, identify direct imaging _cal.fits files from Stage 1\n",
    "# These are typically NrcAllWide, NRCALONG, NRCBLONG, NRCAShort, NRCBSHORT\n",
    "# We'll filter by checking the 'EXP_TYPE' header keyword if available, or by filename conventions.\n",
    "\n",
    "stage1_cal_files = glob.glob(os.path.join(stage1_output_dir, '*_cal.fits')) # or _calints.fits\n",
    "stage1_calints_files = glob.glob(os.path.join(stage1_output_dir, '*_calints.fits'))\n",
    "all_stage1_output_files = list(set(stage1_cal_files + stage1_calints_files))\n",
    "\n",
    "print(f\"Found {len(all_stage1_output_files)} _cal/_calints.fits files in {stage1_output_dir}\")\n",
    "\n",
    "direct_image_cal_files = []\n",
    "wfss_image_cal_files = [] # We'll need these later for Spec2\n",
    "\n",
    "# Example filter: (this might need adjustment based on actual filenames)\n",
    "for f_path in all_stage1_output_files:\n",
    "    filename = os.path.basename(f_path)\n",
    "    # This is a placeholder. Ideally, check EXP_TYPE in FITS header.\n",
    "    # For program 6480, obs 15, direct images are F444W, F356W etc. GRISM is F356WGRISMR etc.\n",
    "    # A simple check for 'grism' or 'wfss' in the filename to distinguish.\n",
    "    if 'grism' in filename.lower() or 'wfss' in filename.lower() or 'grismr' in filename.lower() or 'grismc' in filename.lower():\n",
    "        wfss_image_cal_files.append(f_path)\n",
    "    else:\n",
    "        direct_image_cal_files.append(f_path)\n",
    "\n",
    "print(f\"Identified {len(direct_image_cal_files)} potential direct image _cal/_calints files.\")\n",
    "for f in direct_image_cal_files:\n",
    "    print(f\" - Direct Image: {os.path.basename(f)}\")\n",
    "print(f\"Identified {len(wfss_image_cal_files)} potential WFSS _cal/_calints files.\")\n",
    "for f in wfss_image_cal_files:\n",
    "    print(f\" - WFSS Image: {os.path.basename(f)}\")\n",
    "\n",
    "# Output directory for Stage 2 and 3 products\n",
    "stage2_output_dir = os.path.join(DOWNLOAD_DIR, 'stage2_products')\n",
    "if not os.path.exists(stage2_output_dir):\n",
    "    os.makedirs(stage2_output_dir)\n",
    "    print(f\"Created directory: {stage2_output_dir}\")\n",
    "\n",
    "stage3_output_dir = os.path.join(DOWNLOAD_DIR, 'stage3_products')\n",
    "if not os.path.exists(stage3_output_dir):\n",
    "    os.makedirs(stage3_output_dir)\n",
    "    print(f\"Created directory: {stage3_output_dir}\")\n",
    "    \n",
    "processed_stage2_files = []\n",
    "combined_image_path = None # Initialize here\n",
    "\n",
    "if not direct_image_cal_files:\n",
    "    print(\"No direct image _cal/_calints files found for Stage 2 processing.\")\n",
    "else:\n",
    "    for cal_file in direct_image_cal_files:\n",
    "        print(f\"\\nProcessing direct image file with Image2Pipeline: {os.path.basename(cal_file)}\")\n",
    "        try:\n",
    "            pipeline_stage2 = Image2Pipeline()\n",
    "            pipeline_stage2.output_dir = stage2_output_dir\n",
    "            result = pipeline_stage2.run(cal_file)\n",
    "\n",
    "            base_name = os.path.basename(cal_file).replace('_cal.fits', '_i2d.fits').replace('_calints.fits', '_i2d.fits')\n",
    "            output_i2d_path = os.path.join(stage2_output_dir, base_name)\n",
    "\n",
    "            if os.path.exists(output_i2d_path):\n",
    "                print(f\"Successfully processed with Image2Pipeline. Output: {base_name}\")\n",
    "                processed_stage2_files.append(output_i2d_path)\n",
    "            else:\n",
    "                 print(f\"Warning: Output file {output_i2d_path} not found after Image2Pipeline for {cal_file}. Checking if pipeline returned models.\")\n",
    "                 # Add model saving logic if necessary, example:\n",
    "                 # if result and hasattr(result, 'save'): result.save(output_i2d_path) ...\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {cal_file} with Image2Pipeline: {e}\")\n",
    "\n",
    "print(f\"\\nFound {len(processed_stage2_files)} _i2d.fits files for potential Stage 3 combination.\")\n",
    "\n",
    "if len(processed_stage2_files) > 1:\n",
    "    print(\"\\nAttempting to combine direct images using Image3Pipeline...\")\n",
    "    try:\n",
    "        asn_filename = os.path.join(stage3_output_dir, f\"jw{PROGRAM_ID}_direct_images_asn.json\")\n",
    "        level3_product_name = f\"jw{PROGRAM_ID}_{OBSERVATION}_nircam_direct_combined\"\n",
    "        asn_data = asn_from_list.asn_from_list(processed_stage2_files, rule=DMS_Level3_Base, product_name=level3_product_name)\n",
    "        with open(asn_filename, 'w') as f_asn:\n",
    "            f_asn.write(asn_data.dump_as_json())\n",
    "        print(f\"Generated association file: {asn_filename}\")\n",
    "\n",
    "        pipeline_stage3 = Image3Pipeline()\n",
    "        pipeline_stage3.output_dir = stage3_output_dir\n",
    "        result_s3 = pipeline_stage3.run(asn_filename)\n",
    "        \n",
    "        # Default output from Image3Pipeline is often _i2d.fits for the combined product\n",
    "        expected_combined_file = os.path.join(stage3_output_dir, f\"{level3_product_name}_i2d.fits\") \n",
    "        if not os.path.exists(expected_combined_file):\n",
    "             # Fallback to check for _drz.fits if resample step was run and named it differently\n",
    "             expected_combined_file = os.path.join(stage3_output_dir, f\"{level3_product_name}_drz.fits\")\n",
    "\n",
    "        if os.path.exists(expected_combined_file):\n",
    "            print(f\"Successfully combined images with Image3Pipeline. Output: {os.path.basename(expected_combined_file)}\")\n",
    "            combined_image_path = expected_combined_file\n",
    "        else:\n",
    "            print(f\"Error: Combined image {os.path.basename(expected_combined_file)} not found after Image3Pipeline.\")\n",
    "            print(f\"Pipeline result: {result_s3}\")\n",
    "            combined_image_path = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Image3Pipeline: {e}\")\n",
    "        combined_image_path = None\n",
    "elif len(processed_stage2_files) == 1:\n",
    "    combined_image_path = processed_stage2_files[0]\n",
    "    print(f\"Only one direct image processed through Stage 2. Using this for segmentation: {os.path.basename(combined_image_path)}\")\n",
    "else:\n",
    "    print(\"No Stage 2 processed direct images available for segmentation.\")\n",
    "    combined_image_path = None\n",
    "\n",
    "print(\"\\n--- Image Alignment and Combination Summary ---\")\n",
    "if combined_image_path and os.path.exists(combined_image_path):\n",
    "    print(f\"Final direct image for segmentation: {os.path.basename(combined_image_path)} in {os.path.dirname(combined_image_path)}\")\n",
    "else:\n",
    "    print(\"Image alignment and combination did not produce a final image for segmentation.\")\n",
    "\n",
    "# Store the path to the combined/final direct image and WFSS cal files for the next steps\n",
    "try:\n",
    "    get_ipython().run_line_magic('store', 'combined_image_path')\n",
    "    get_ipython().run_line_magic('store', 'wfss_image_cal_files')\n",
    "    print(\"Stored combined_image_path and wfss_image_cal_files for next steps.\")\n",
    "except NameError:\n",
    "    print(\"Could not store variables - not in IPython environment. Ensure variables are passed correctly to next cells if running as script.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Segmentation Map\n",
    "\n",
    "This section uses the final combined/processed direct image from the previous step to create a segmentation map.\n",
    "The `photutils` library is used to detect sources in the image.\n",
    "The process involves:\n",
    "1. Loading the science data from the FITS file.\n",
    "2. Optionally, performing background estimation and subtraction (currently commented out).\n",
    "3. Optionally, smoothing the image (currently commented out).\n",
    "4. Detecting sources using `photutils.segmentation.detect_sources` based on a sigma-clipped threshold.\n",
    "5. Saving the resulting segmentation map as a new FITS file.\n",
    "This segmentation map is crucial for identifying the locations of sources for spectral extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.segmentation import detect_sources, SegmentationImage\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "import numpy as np\n",
    "import glob # Ensure glob is imported if used in fallback\n",
    "import os # Ensure os is imported\n",
    "\n",
    "# --- Create Segmentation Map ---\n",
    "segmentation_map_path = None # Initialize\n",
    "retrieved_combined_image_path = None\n",
    "\n",
    "try:\n",
    "    # Attempt to retrieve the stored variable\n",
    "    ipy = get_ipython()\n",
    "    if 'combined_image_path' in ipy.user_ns:\n",
    "        retrieved_combined_image_path = ipy.user_ns['combined_image_path']\n",
    "    else:\n",
    "        print(\"Warning: 'combined_image_path' not found in IPython user_ns. Trying %store -r.\")\n",
    "        ipy.run_line_magic('store', '-r combined_image_path')\n",
    "        if 'combined_image_path' in ipy.user_ns:\n",
    "            retrieved_combined_image_path = ipy.user_ns['combined_image_path']\n",
    "        else:\n",
    "            print(\"Error: 'combined_image_path' still not found after %store -r.\")\n",
    "\n",
    "    if retrieved_combined_image_path is None or not os.path.exists(retrieved_combined_image_path):\n",
    "        print(f\"Error: combined_image_path ('{retrieved_combined_image_path}') is not available or does not exist.\")\n",
    "        # Fallback search logic\n",
    "        fallback_search_paths = []\n",
    "        if 'stage3_output_dir' in locals() or 'stage3_output_dir' in globals(): # Check if var exists\n",
    "            fallback_search_paths.extend([\n",
    "                glob.glob(os.path.join(stage3_output_dir, '*_i2d.fits')),\n",
    "                glob.glob(os.path.join(stage3_output_dir, '*_drz.fits')),\n",
    "            ])\n",
    "        if 'stage2_output_dir' in locals() or 'stage2_output_dir' in globals():\n",
    "             fallback_search_paths.append(glob.glob(os.path.join(stage2_output_dir, '*_i2d.fits')))\n",
    "        \n",
    "        found_fallback = False\n",
    "        for path_list in fallback_search_paths:\n",
    "            if path_list:\n",
    "                retrieved_combined_image_path = path_list[0]\n",
    "                print(f\"Using fallback image for segmentation: {retrieved_combined_image_path}\")\n",
    "                found_fallback = True\n",
    "                break\n",
    "        if not found_fallback:\n",
    "             print(\"Critical Error: No valid combined direct image found from primary or fallback paths.\")\n",
    "             # To prevent further execution of this cell if no image, could raise ValueError here\n",
    "             # For now, script will continue and subsequent 'if' will handle it\n",
    "    \n",
    "except Exception as e: # Catching broader exceptions during retrieval including NameError if get_ipython fails\n",
    "    print(f\"Error during variable retrieval: {e}. Check if previous cells ran and if running in IPython.\")\n",
    "    # Fallback search if retrieval fails catastrophically\n",
    "    if retrieved_combined_image_path is None: # If still None\n",
    "        print(\"Attempting fallback search for image due to retrieval error...\")\n",
    "        # (Add fallback search logic here too, similar to above, ensure dirs are defined)\n",
    "        # This is redundant if the above NameError for combined_image_path itself is caught,\n",
    "        # but good for robustness if other errors occur in try block.\n",
    "\n",
    "\n",
    "if retrieved_combined_image_path and os.path.exists(retrieved_combined_image_path):\n",
    "    print(f\"Creating segmentation map from: {os.path.basename(retrieved_combined_image_path)}\")\n",
    "\n",
    "    try:\n",
    "        with fits.open(retrieved_combined_image_path) as hdul:\n",
    "            sci_ext_name = 'SCI'\n",
    "            image_data = None\n",
    "            if sci_ext_name in hdul:\n",
    "                image_data = hdul[sci_ext_name].data\n",
    "            else:\n",
    "                for ext in hdul:\n",
    "                    if ext.is_image_hdu and ext.data is not None and (ext.name == 'DATA' or ext.name == 'PRIMARY' or ext.name == ''):\n", # Broader search
    "                        print(f\"Warning: 'SCI' extension not found. Using extension '{ext.name}'.\")\n",
    "                        image_data = ext.data\n",
    "                        break\n",
    "                if image_data is None:\n",
    "                    raise ValueError(f\"Could not find a suitable science data extension in {retrieved_combined_image_path}\")\n",
    "            \n",
    "            if np.any(np.isnan(image_data)):\n",
    "                print(\"Warning: NaNs found in image data. Replacing with global median for segmentation.\")\n",
    "                image_data = np.nan_to_num(image_data, nan=np.nanmedian(image_data))\n",
    "\n",
    "        data_for_segmentation = image_data\n",
    "        mean, median, std = sigma_clipped_stats(data_for_segmentation, sigma=3.0)\n",
    "        threshold = median + (3.0 * std)\n",
    "        print(f\"Segmentation threshold: {threshold:.4f} (based on median={median:.4f}, std={std:.4f})\")\n",
    "        \n",
    "        if not np.isscalar(threshold):\n",
    "            print(f\"Warning: Threshold is not a scalar ({type(threshold)}). Using its mean value.\")\n",
    "            threshold = np.mean(threshold)\n",
    "\n",
    "        segment_map = detect_sources(data_for_segmentation, threshold, npixels=10)\n",
    "\n",
    "        if segment_map:\n",
    "            print(f\"Segmentation complete. Found {segment_map.nlabels} sources.\")\n",
    "            segm_hdu = fits.ImageHDU(data=segment_map.data.astype(np.int32), name='SEGMAP')\n",
    "            primary_hdu = fits.PrimaryHDU()\n",
    "            hdul_segm = fits.HDUList([primary_hdu, segm_hdu])\n",
    "            \n",
    "            # Ensure stage3_output_dir is defined, fallback to DOWNLOAD_DIR or current dir\n",
    "            output_dir_segm = stage3_output_dir if ('stage3_output_dir' in locals() or 'stage3_output_dir' in globals()) and os.path.exists(stage3_output_dir) else DOWNLOAD_DIR\n",
    "            if not (os.path.exists(output_dir_segm)):\n",
    "                output_dir_segm = '.' # Current directory as last resort\n",
    "                print(f\"Warning: Stage 3 output directory not found. Saving segmentation map to current directory: {output_dir_segm}\")\n",
    "            \n",
    "            segmentation_map_filename = os.path.basename(retrieved_combined_image_path).replace('.fits', '_segm.fits')\n",
    "            segmentation_map_path = os.path.join(output_dir_segm, segmentation_map_filename)\n",
    "            \n",
    "            hdul_segm.writeto(segmentation_map_path, overwrite=True)\n",
    "            print(f\"Segmentation map saved to: {segmentation_map_path}\")\n",
    "        else:\n",
    "            print(\"No sources detected, or segmentation failed. Segmentation map not saved.\")\n",
    "            segmentation_map_path = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating segmentation map: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        segmentation_map_path = None\n",
    "else:\n",
    "    print(\"Skipping segmentation map creation as no valid combined direct image path is available.\")\n",
    "\n",
    "# Store the path to the segmentation map for the next step\n",
    "try:\n",
    "    get_ipython().run_line_magic('store', 'segmentation_map_path')\n",
    "    print(f\"Stored segmentation_map_path: {segmentation_map_path}\")\n",
    "except NameError:\n",
    "    print(\"Could not store segmentation_map_path - not in IPython environment.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 1D Spectral Extraction (Spec2Pipeline and Contamination Considerations)\n",
    "\n",
    "This section processes the WFSS _cal.fits files (or _calints.fits) using the JWST Stage 2 Spectroscopic Pipeline (`Spec2Pipeline`).\n",
    "The key goals here are:\n",
    "- Perform standard spectroscopic calibrations (WCS assignment, flat-fielding).\n",
    "- Produce calibrated 2D spectral images (_s2d.fits).\n",
    "\n",
    "**Important Considerations for NIRCam WFSS 1D Extraction:**\n",
    "- **Contamination:** Wide-Field Slitless Spectroscopy is prone to spectral overlap from different sources. Accurately extracting a 1D spectrum for a target requires modeling and accounting for this contamination.\n",
    "- **Source Catalog:** A catalog of source positions (derived from the segmentation map created earlier) is essential. These positions need to be transformed into the coordinate system of each WFSS exposure.\n",
    "- **Specialized Tools:** While `Spec2Pipeline` includes an `extract_1d` step, robust 1D extraction for WFSS often necessitates specialized software (e.g., Grizli, MSAX) or custom scripts that can perform detailed contamination modeling and optimal extraction.\n",
    "\n",
    "**Current Implementation:**\n",
    "- This notebook runs `Spec2Pipeline` primarily to produce the `_s2d.fits` files.\n",
    "- The `extract_1d` step within `Spec2Pipeline` is currently **skipped** (`extract_1d.skip = True`). This is because robust 1D extraction without proper contamination handling (which is complex) can be misleading.\n",
    "- The markdown output below this cell (now part of this markdown cell) provides an overview of the typical workflow needed for contamination modeling and 1D extraction:\n",
    "\n",
    "For robust 1D spectral extraction from NIRCam WFSS data, especially in crowded fields,\n",
    "the following steps are typically required after running Spec2Pipeline:\n",
    "\n",
    "1.  **Source Catalog from Segmentation Map:**\n",
    "    Convert the segmentation map into a source catalog. This catalog should at least contain accurate X, Y positions\n",
    "    of all detected sources in the direct image coordinate system.\n",
    "    Tools: `photutils.source_properties` or custom scripts.\n",
    "\n",
    "2.  **Transform Catalog to WFSS Frame(s):**\n",
    "    Transform these direct image coordinates to the coordinate system of each individual\n",
    "    WFSS exposure (_s2d.fits files). This requires accurate WCS information for both\n",
    "    direct images and WFSS exposures.\n",
    "\n",
    "3.  **Contamination Modeling:**\n",
    "    For each source of interest, model the contribution of dispersed light from all\n",
    "    other sources (contaminators) at the location of the target's spectrum.\n",
    "    This is the most complex step and often requires specialized tools such as:\n",
    "    - `grizli` (https://github.com/gbrammer/grizli)\n",
    "    - `MSAX` (https://github.com/spacetelescope/msax)\n",
    "    - Custom Python scripts utilizing spectral trace models and source properties.\n",
    "    The choice of tool depends on the complexity of the field and desired accuracy.\n",
    "\n",
    "4.  **Optimal 1D Spectral Extraction:**\n",
    "    Once the contamination is modeled (and potentially subtracted or accounted for in the fit),\n",
    "    extract the 1D spectrum for each target. This can be done using:\n",
    "    - The `jwst.extract_1d.Extract1dStep` if provided with a good contamination model\n",
    "      (e.g., by passing a modified 2D spectrum).\n",
    "    - Algorithms like optimal extraction (Horne 1986) implemented in custom scripts or packages.\n",
    "\n",
    "Due to the complexity and potential need for external software/detailed configuration for\n",
    "contamination modeling and robust 1D extraction, implementing this fully is beyond\n",
    "the scope of a single automated notebook cell without specific tool choices.\n",
    "\n",
    "The `_s2d.fits` files generated by `Spec2Pipeline` are the primary input for these advanced steps.\n",
    "We will proceed to spectroscopic redshift fitting using any `_x1d.fits` files that *might* have been\n",
    "produced by `Spec2Pipeline` (if `extract_1d.skip` was `False` and it ran successfully by default),\n",
    "or placeholder/example spectra if not. A proper workflow would insert the detailed\n",
    "contamination modeling and extraction here.\n",
    "\n",
    "- The paths to any `_s2d.fits` files produced are stored for potential use with external tools. We also check if any `_x1d.fits` files were inadvertently produced to inform the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst.pipeline import Spec2Pipeline\n",
    "import glob \n",
    "import os \n",
    "\n",
    "# --- 1D Spectral Extraction (via Spec2Pipeline for _s2d.fits) ---\n",
    "retrieved_wfss_image_cal_files = None\n",
    "retrieved_segmentation_map_path = None\n",
    "\n",
    "try:\n",
    "    ipy = get_ipython()\n",
    "    if 'wfss_image_cal_files' in ipy.user_ns:\n",
    "        retrieved_wfss_image_cal_files = ipy.user_ns['wfss_image_cal_files']\n",
    "    else:\n",
    "        print(\"Warning: 'wfss_image_cal_files' not in IPython user_ns. Trying %store -r.\")\n",
    "        ipy.run_line_magic('store', '-r wfss_image_cal_files')\n",
    "        retrieved_wfss_image_cal_files = ipy.user_ns.get('wfss_image_cal_files')\n",
    "\n",
    "    if retrieved_wfss_image_cal_files is None:\n",
    "        raise NameError(\"'wfss_image_cal_files' not successfully restored.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error restoring wfss_image_cal_files: {e}. Attempting fallback.\")\n",
    "    retrieved_wfss_image_cal_files = []\n",
    "    # Check if stage1_output_dir is defined and exists\n",
    "    if ('stage1_output_dir' in locals() or 'stage1_output_dir' in globals()) and os.path.exists(stage1_output_dir):\n",
    "        temp_stage1_cal_files = glob.glob(os.path.join(stage1_output_dir, '*_cal.fits'))\n",
    "        temp_stage1_calints_files = glob.glob(os.path.join(stage1_output_dir, '*_calints.fits'))\n",
    "        all_s1_files = list(set(temp_stage1_cal_files + temp_stage1_calints_files))\n",
    "        retrieved_wfss_image_cal_files = [f for f in all_s1_files if 'grism' in os.path.basename(f).lower() or 'wfss' in os.path.basename(f).lower()] # Match on basename\n",
    "    if not retrieved_wfss_image_cal_files:\n",
    "        print(\"Critical Error: Could not load or find wfss_image_cal_files via primary or fallback.\")\n",
    "    else:\n",
    "        print(f\"Found {len(retrieved_wfss_image_cal_files)} WFSS cal files via fallback glob.\")\n",
    "\n",
    "try:\n",
    "    ipy = get_ipython()\n",
    "    if 'segmentation_map_path' in ipy.user_ns:\n",
    "        retrieved_segmentation_map_path = ipy.user_ns['segmentation_map_path']\n",
    "    else:\n",
    "        print(\"Warning: 'segmentation_map_path' not in IPython user_ns. Trying %store -r.\")\n",
    "        ipy.run_line_magic('store', '-r segmentation_map_path')\n",
    "        retrieved_segmentation_map_path = ipy.user_ns.get('segmentation_map_path')\n",
    "\n",
    "    if retrieved_segmentation_map_path is None or not os.path.exists(retrieved_segmentation_map_path):\n",
    "        raise NameError(\"'segmentation_map_path' not successfully restored or path invalid.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error restoring segmentation_map_path: {e}. Attempting fallback.\")\n",
    "    retrieved_segmentation_map_path = None\n",
    "    if ('stage3_output_dir' in locals() or 'stage3_output_dir' in globals()) and os.path.exists(stage3_output_dir):\n",
    "        potential_segmaps = glob.glob(os.path.join(stage3_output_dir, '*_segm.fits'))\n",
    "        if potential_segmaps:\n",
    "            retrieved_segmentation_map_path = potential_segmaps[0]\n",
    "            print(f\"Using fallback segmentation map: {retrieved_segmentation_map_path}\")\n",
    "    if not retrieved_segmentation_map_path:\n",
    "        print(\"Warning: Segmentation map path could not be loaded or found via primary or fallback.\")\n",
    "\n",
    "spec2_output_dir = os.path.join(DOWNLOAD_DIR, 'spec2_products')\n",
    "if not os.path.exists(spec2_output_dir):\n",
    "    os.makedirs(spec2_output_dir)\n",
    "    print(f\"Created directory: {spec2_output_dir}\")\n",
    "\n",
    "processed_spec2_files = [] \n",
    "\n",
    "if not retrieved_wfss_image_cal_files:\n",
    "    print(\"No WFSS _cal/_calints files found to process with Spec2Pipeline.\")\n",
    "else:\n",
    "    print(f\"Found {len(retrieved_wfss_image_cal_files)} WFSS _cal/_calints files for Spec2Pipeline processing:\")\n",
    "    for f_path in retrieved_wfss_image_cal_files:\n",
    "        print(f\" - {os.path.basename(f_path)}\")\n",
    "\n",
    "    for wfss_cal_file in retrieved_wfss_image_cal_files:\n",
    "        print(f\"\\nProcessing WFSS file with Spec2Pipeline: {os.path.basename(wfss_cal_file)}\")\n",
    "        try:\n",
    "            pipeline_spec2 = Spec2Pipeline()\n",
    "            pipeline_spec2.output_dir = spec2_output_dir\n",
    "            \n",
    "            # Key settings for WFSS processing to get _s2d.fits primarily\n",
    "            pipeline_spec2.assign_wcs.skip = False\n",
    "            pipeline_spec2.flat_field.skip = False\n",
    "            pipeline_spec2.pathloss.skip = True # Pathloss correction is complex for WFSS, often handled by specialized tools\n",
    "            pipeline_spec2.photom.skip = False # Apply flux calibration\n",
    "            \n",
    "            # For NIRCam WFSS, extract_2d is the step that produces the s2d files from cal/calints.\n",
    "            pipeline_spec2.extract_2d.skip = False \n",
    "            \n",
    "            # As per instructions, skip 1D extraction within Spec2Pipeline due to contamination\n",
    "            pipeline_spec2.extract_1d.skip = True \n",
    "            \n",
    "            # If a segmentation map is available, it can be used by some steps (e.g. master_background)\n",
    "            # However, direct use for contamination-mitigated extraction is usually in extract_1d or specialized tools.\n",
    "            # For now, we're just ensuring it's available if needed, but not explicitly passing to a specific step here\n",
    "            # unless a step like master_background or background is configured to use it.\n",
    "            # pipeline_spec2.master_background.source_catname = retrieved_segmentation_map_path # Example if using this step\n",
    "\n",
    "            result_spec2 = pipeline_spec2.run(wfss_cal_file)\n",
    "            \n",
    "            base_name_in = os.path.basename(wfss_cal_file)\n",
    "            if base_name_in.endswith('_cal.fits'):\n",
    "                base_name_out_s2d = base_name_in.replace('_cal.fits', '_s2d.fits')\n",
    "            elif base_name_in.endswith('_calints.fits'):\n",
    "                base_name_out_s2d = base_name_in.replace('_calints.fits', '_s2d.fits')\n",
    "            else:\n",
    "                print(f\"Warning: Unrecognized suffix for input file {base_name_in}. Assuming generic output name.\")\n",
    "                base_name_out_s2d = os.path.splitext(base_name_in)[0] + '_s2d.fits'\n",
    "            \n",
    "            output_s2d_path = os.path.join(spec2_output_dir, base_name_out_s2d)\n",
    "\n",
    "            # Check for _s2d.fits (primary 2D product)\n",
    "            if os.path.exists(output_s2d_path):\n",
    "                print(f\"Successfully processed. Standard output: {base_name_out_s2d}\")\n",
    "                if output_s2d_path not in processed_spec2_files:\n",
    "                    processed_spec2_files.append(output_s2d_path)\n",
    "            else:\n",
    "                print(f\"Warning: Expected standard output _s2d.fits file {output_s2d_path} not found.\")\n",
    "            \n",
    "            # The pipeline.run() method can return a DataModel or ModelContainer\n",
    "            # We should check what it returns and save any models if they were not written by the pipeline steps.\n",
    "            # This is crucial if pipeline steps are run with `save_results=False` for some reason,\n",
    "            # or if the final product is held in memory.\n",
    "            if result_spec2 is not None:\n",
    "                if isinstance(result_spec2, list): # If it's a list of models (e.g. ModelContainer)\n",
    "                    for model in result_spec2:\n",
    "                        if hasattr(model, 'meta') and hasattr(model.meta, 'filename') and model.meta.filename:\n",
    "                            # Typically, s2d, x1d (if not skipped) etc.\n",
    "                            # Save to the designated output directory\n",
    "                            current_model_filename = model.meta.filename\n",
    "                            current_model_path = os.path.join(spec2_output_dir, current_model_filename)\n",
    "                            try:\n",
    "                                model.save(current_model_path, overwrite=True)\n",
    "                                print(f\"  Saved/Resaved model from pipeline result to: {current_model_path}\")\n",
    "                                if current_model_path not in processed_spec2_files and current_model_filename.endswith('_s2d.fits'):\n",
    "                                    processed_spec2_files.append(current_model_path)\n",
    "                            except Exception as save_e:\n",
    "                                print(f\"  Error saving model {current_model_filename} from pipeline result: {save_e}\")\n",
    "                elif hasattr(result_spec2, 'meta') and hasattr(result_spec2.meta, 'filename') and result_spec2.meta.filename: # Single DataModel\n",
    "                    current_model_filename = result_spec2.meta.filename\n",
    "                    current_model_path = os.path.join(spec2_output_dir, current_model_filename)\n",
    "                    try:\n",
    "                        result_spec2.save(current_model_path, overwrite=True)\n",
    "                        print(f\"  Saved/Resaved single model from pipeline result to: {current_model_path}\")\n",
    "                        if current_model_path not in processed_spec2_files and current_model_filename.endswith('_s2d.fits'):\n",
    "                            processed_spec2_files.append(current_model_path)\n",
    "                    except Exception as save_e:\n",
    "                        print(f\"  Error saving single model {current_model_filename} from pipeline result: {save_e}\")\n",
    "                else:\n",
    "                    print(f\"  Pipeline result for {base_name_in} is not a recognized model or list of models, or has no filename metadata.\")\n",
    "            else:\n",
    "                 print(f\"  Pipeline run for {base_name_in} returned None. Check logs for errors.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {wfss_cal_file} with Spec2Pipeline: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Spec2Pipeline Processing Summary ---\")\n",
    "processed_spec2_files = sorted(list(set(processed_spec2_files))) # Unique and sort\n",
    "if processed_spec2_files:\n",
    "    print(\"Successfully processed/saved the following primary products from Spec2Pipeline (e.g. _s2d.fits):\")\n",
    "    for f_s2d in processed_spec2_files:\n",
    "        print(f\" - {os.path.basename(f_s2d)} in {os.path.dirname(f_s2d)}\")\n",
    "else:\n",
    "    print(\"No WFSS files were successfully processed by Spec2Pipeline or no output files were found/saved.\")\n",
    "\n",
    "# Store variables for the next step\n",
    "try:\n",
    "    ipy = get_ipython()\n",
    "    if processed_spec2_files: # Only store if list is not empty\n",
    "        ipy.user_ns['processed_spec2_files'] = processed_spec2_files\n",
    "        ipy.run_line_magic('store', 'processed_spec2_files')\n",
    "        print(f\"Stored processed_spec2_files: {processed_spec2_files}\")\n",
    "    if retrieved_segmentation_map_path: # Only store if path is valid\n",
    "        ipy.user_ns['segmentation_map_path_spec2'] = retrieved_segmentation_map_path # Use a different name to avoid conflict if needed\n",
    "        ipy.run_line_magic('store', 'segmentation_map_path_spec2')\n",
    "        print(f\"Stored segmentation_map_path_spec2: {retrieved_segmentation_map_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not store variables using IPython: {e}\")\n",
    "\n",
    "extracted_1d_spectra_paths = []\n",
    "if ('spec2_output_dir' in locals() or 'spec2_output_dir' in globals()) and os.path.exists(spec2_output_dir):\n",
    "    for root_dir, sub_dirs, files_list in os.walk(spec2_output_dir):\n",
    "        for file_item in files_list:\n",
    "            if file_item.endswith('_x1d.fits'): # Check for 1D extracted spectra\n",
    "                extracted_1d_spectra_paths.append(os.path.join(root_dir, file_item))\n",
    "\n",
    "if extracted_1d_spectra_paths:\n",
    "    print(\"\\nFound the following _x1d.fits files (1D extracted spectra from Spec2Pipeline, if extract_1d was not skipped):\")\n",
    "    for x1d_file_path in extracted_1d_spectra_paths:\n",
    "        print(f\" - {x1d_file_path}\")\n",
    "else:\n",
    "    print(\"\\nNo _x1d.fits files found from Spec2Pipeline (as extract_1d was intentionally skipped or did not produce them).\")\n",
    "    print(\"This is expected. Redshift fitting will require either manual creation of 1D spectra after contamination modeling,\")\n",
    "    print(\"or using placeholder/example spectra for the next step.\")\n",
    "\n",
    "try:\n",
    "    ipy = get_ipython()\n",
    "    if extracted_1d_spectra_paths: # Only store if list is not empty\n",
    "        ipy.user_ns['extracted_1d_spectra_paths'] = extracted_1d_spectra_paths\n",
    "        ipy.run_line_magic('store', 'extracted_1d_spectra_paths')\n",
    "        print(f\"Stored extracted_1d_spectra_paths: {extracted_1d_spectra_paths}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not store extracted_1d_spectra_paths using IPython: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spectroscopic Redshift Fitting (Demonstration)\n",
    "\n",
    "This section demonstrates how spectroscopic redshift fitting could be performed on 1D extracted spectra.\n",
    "Since robust 1D spectral extraction with contamination modeling for WFSS data is complex and typically requires specialized tools (as discussed in the previous section), this part focuses on the principles of redshift fitting assuming you have a clean 1D spectrum.\n",
    "\n",
    "**Workflow Demonstrated:**\n",
    "1.  **Load 1D Spectra:**\n",
    "    - The code attempts to load any `_x1d.fits` files that might have been stored from previous steps (though in our specific pipeline configuration, `extract_1d` was skipped in `Spec2Pipeline`, so this list is expected to be empty).\n",
    "    - **If no real 1D spectra are found, a placeholder `Spectrum1D` object is created.** This dummy spectrum includes a continuum and a few simulated emission lines at a known redshift (z=3) to illustrate the fitting process.\n",
    "2.  **Continuum Subtraction (Example):**\n",
    "    - A simple polynomial continuum is fitted to the spectrum and subtracted. This is often a pre-requisite for accurate line identification or cross-correlation.\n",
    "3.  **Cross-Correlation (Example):**\n",
    "    - A basic cross-correlation is performed against a simple template spectrum (e.g., a Gaussian representing a common emission line like H-alpha).\n",
    "    - The peak of the cross-correlation function across a range of trial redshifts gives an estimate of the source's redshift.\n",
    "    - *Note: Real-world cross-correlation uses extensive template libraries and more sophisticated algorithms.*\n",
    "4.  **Emission Line Fitting (Conceptual):**\n",
    "    - The code includes commented-out examples of how one might fit Gaussian profiles to identified emission lines using `specutils.fitting.fit_lines`.\n",
    "    - The observed wavelength of a fitted line, compared to its known rest-frame wavelength, directly yields a redshift.\n",
    "\n",
    "**Important Notes:**\n",
    "- **Units:** Handling spectral flux and wavelength units correctly is critical. The demo uses `astropy.units`. Real JWST `_x1d.fits` files have specific unit conventions (e.g., wavelength in microns, flux in MJy/sr or similar) that must be carefully parsed and converted if necessary.\n",
    "- **Templates:** For cross-correlation, a comprehensive library of template spectra (galaxies, quasars, stars) is essential for reliable results.\n",
    "- **Line Identification:** For emission/absorption line fitting, correctly identifying the lines (e.g., Lyman-alpha, H-alpha, [OIII], [OII], Ca H&K) is key.\n",
    "- **Complexity:** This is a simplified demonstration. Robust redshift fitting often involves iterative processes, consideration of signal-to-noise, and sometimes visual inspection or Bayesian approaches.\n",
    "\n",
    "The plots generated (placeholder spectrum, continuum subtraction, cross-correlation) are saved in the `redshift_fitting_products` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.wcs import WCS\n",
    "from astropy.modeling import models\n",
    "from astropy.visualization import quantity_support\n",
    "from specutils import Spectrum1D, SpectralRegion\n",
    "from specutils.fitting import fit_lines, fit_continuum\n",
    "# from specutils.analysis import correlation # Current specutils might not have this directly, or it's in sub-modules\n",
    "from specutils.manipulation import extract_region # Corrected import\n",
    "import astropy.units as u # Ensure units are imported\n",
    "import os # Ensure os is imported\n",
    "\n",
    "# --- Spectroscopic Redshift Fitting ---\n",
    "retrieved_extracted_1d_spectra_paths = [] # Initialize\n",
    "try:\n",
    "    ipy = get_ipython()\n",
    "    if 'extracted_1d_spectra_paths' in ipy.user_ns:\n",
    "        retrieved_extracted_1d_spectra_paths = ipy.user_ns['extracted_1d_spectra_paths']\n",
    "    else:\n",
    "        print(\"Warning: 'extracted_1d_spectra_paths' not in IPython user_ns. Trying %store -r.\")\n",
    "        ipy.run_line_magic('store', '-r extracted_1d_spectra_paths')\n",
    "        retrieved_extracted_1d_spectra_paths = ipy.user_ns.get('extracted_1d_spectra_paths', []) # Default to empty list\n",
    "    \n",
    "    if retrieved_extracted_1d_spectra_paths is None: # Handle case where %store -r might return None explicitly\n",
    "        retrieved_extracted_1d_spectra_paths = []\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not restore extracted_1d_spectra_paths: {e}. Proceeding with empty list.\")\n",
    "    retrieved_extracted_1d_spectra_paths = []\n",
    "\n",
    "redshift_fitting_output_dir = os.path.join(DOWNLOAD_DIR, 'redshift_fitting_products')\n",
    "if not os.path.exists(redshift_fitting_output_dir):\n",
    "    os.makedirs(redshift_fitting_output_dir)\n",
    "    print(f\"Created directory: {redshift_fitting_output_dir}\")\n",
    "\n",
    "spectra_to_fit = {}\n",
    "\n",
    "if not retrieved_extracted_1d_spectra_paths:\n",
    "    print(\"\\nNo pre-existing _x1d.fits files found or restored. Redshift fitting will be demonstrated with a placeholder spectrum.\")\n",
    "    wavelength_microns = np.linspace(2.4, 5.0, 500) * u.micron \n",
    "    continuum_level = 1.0 * u.uJy\n",
    "    z_sim = 3.0\n",
    "    line1_rest_um = 0.6563 * u.micron\n",
    "    line1_obs_um = line1_rest_um * (1 + z_sim)\n",
    "    line1_amp = 5.0 * u.uJy\n",
    "    line1_stddev_um = 0.005 * u.micron\n",
    "    line2_rest_um = 0.5007 * u.micron\n",
    "    line2_obs_um = line2_rest_um * (1 + z_sim)\n",
    "    line2_amp = 7.0 * u.uJy\n",
    "    line2_stddev_um = 0.005 * u.micron\n",
    "    flux_values = continuum_level.value * np.ones(wavelength_microns.shape)\n",
    "    flux_values += line1_amp.value * np.exp(-0.5 * ((wavelength_microns.value - line1_obs_um.value) / line1_stddev_um.value)**2) # Corrected stddev variable\n",
    "    flux_values += line2_amp.value * np.exp(-0.5 * ((wavelength_microns.value - line2_obs_um.value) / line2_stddev_um.value)**2)\n",
    "    noise = np.random.normal(0, 0.2 * continuum_level.value, size=wavelength_microns.shape)\n",
    "    flux_values += noise\n",
    "    flux_sim = flux_values * u.uJy\n",
    "    placeholder_spectrum = Spectrum1D(flux=flux_sim, spectral_axis=wavelength_microns)\n",
    "    print(f\"Using placeholder spectrum with simulated H-alpha and [OIII] at z={z_sim}.\")\n",
    "    with quantity_support():\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(placeholder_spectrum.spectral_axis, placeholder_spectrum.flux)\n",
    "        plt.xlabel(f\"Wavelength ({placeholder_spectrum.spectral_axis.unit})\")\n",
    "        plt.ylabel(f\"Flux ({placeholder_spectrum.flux.unit})\")\n",
    "        plt.title(\"Placeholder 1D Spectrum for Redshift Fitting Demo\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(redshift_fitting_output_dir, \"placeholder_spectrum.png\"))\n",
    "        # plt.show() # Avoid blocking in automated environments\n",
    "        plt.close() # Close plot to free memory\n",
    "    spectra_to_fit['placeholder_z_sim_3.0'] = placeholder_spectrum\n",
    "else:\n",
    "    print(f\"\\nFound {len(retrieved_extracted_1d_spectra_paths)} _x1d.fits files to attempt loading for redshift fitting:\")\n",
    "    for x1d_path in retrieved_extracted_1d_spectra_paths:\n",
    "        try:\n",
    "            print(f\"Attempting to load: {os.path.basename(x1d_path)}\")\n",
    "            # This is a known simplification. Real x1d files need careful parsing.\n",
    "            # Example: Spectrum1D.read(x1d_path, format='JWST x1d') or manual HDU parsing.\n",
    "            # For now, we'll assume a simple single-extension FITS for any x1d files found,\n",
    "            # which is unlikely to be correct for actual JWST products without more specific loaders.\n",
    "            temp_spec = Spectrum1D.read(x1d_path) \n",
    "            spectra_to_fit[os.path.basename(x1d_path)] = temp_spec\n",
    "            print(f\"Successfully loaded {os.path.basename(x1d_path)} (units/extensions may need verification).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load or parse {os.path.basename(x1d_path)} as Spectrum1D: {e}. It may require specific loader arguments or manual parsing.\")\n",
    "\n",
    "if not spectra_to_fit:\n",
    "    print(\"\\nNo spectra available (real or placeholder) to demonstrate redshift fitting.\")\n",
    "else:\n",
    "    for name, spectrum_obs in spectra_to_fit.items():\n",
    "        print(f\"\\n--- Attempting Redshift Fit for: {name} ---\")\n",
    "        \n",
    "        # Ensure spectrum has units for specutils operations\n",
    "        if not hasattr(spectrum_obs.spectral_axis, 'unit') or spectrum_obs.spectral_axis.unit is None:\n",
    "            print(f\"Warning: Spectral axis for {name} has no units or unit is None. Assuming microns for demo.\")\n",
    "            spectrum_obs = Spectrum1D(flux=spectrum_obs.flux, spectral_axis=spectrum_obs.spectral_axis.value * u.micron)\n",
    "        if not hasattr(spectrum_obs.flux, 'unit') or spectrum_obs.flux.unit is None:\n",
    "            print(f\"Warning: Flux for {name} has no units or unit is None. Assuming uJy for demo.\")\n",
    "            spectrum_obs = Spectrum1D(flux=spectrum_obs.flux.value * u.uJy, spectral_axis=spectrum_obs.spectral_axis)\n",
    "\n",
    "        try:\n",
    "            if name.startswith(\"placeholder\"):\n",
    "                exclude_region1 = SpectralRegion((line1_obs_um - 0.05*u.micron), (line1_obs_um + 0.05*u.micron))\n",
    "                exclude_region2 = SpectralRegion((line2_obs_um - 0.05*u.micron), (line2_obs_um + 0.05*u.micron))\n",
    "                g1_fit = fit_continuum(spectrum_obs, window=[exclude_region1, exclude_region2], model=models.Linear1D())\n",
    "            else:\n",
    "                g1_fit = fit_continuum(spectrum_obs, model=models.Linear1D())\n",
    "            continuum_fit = g1_fit(spectrum_obs.spectral_axis)\n",
    "            spectrum_continuum_subtracted = spectrum_obs - continuum_fit\n",
    "            \n",
    "            with quantity_support():\n",
    "                plt.figure(figsize=(10,6))\n",
    "                plt.subplot(2,1,1)\n",
    "                plt.plot(spectrum_obs.spectral_axis, spectrum_obs.flux, label=\"Original Spectrum\")\n",
    "                plt.plot(spectrum_obs.spectral_axis, continuum_fit, label=\"Continuum Fit\")\n",
    "                plt.title(f\"Continuum Fit for {name}\")\n",
    "                plt.legend(); plt.grid(True)\n",
    "                plt.subplot(2,1,2)\n",
    "                plt.plot(spectrum_continuum_subtracted.spectral_axis, spectrum_continuum_subtracted.flux, label=\"Continuum Subtracted\")\n",
    "                plt.title(\"Continuum Subtracted Spectrum\")\n",
    "                plt.xlabel(f\"Wavelength ({spectrum_continuum_subtracted.spectral_axis.unit})\")\n",
    "                plt.ylabel(f\"Flux ({spectrum_continuum_subtracted.flux.unit})\")\n",
    "                plt.legend(); plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(redshift_fitting_output_dir, f\"{name}_continuum_subtraction.png\"))\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not perform continuum subtraction for {name}: {e}\")\n",
    "            spectrum_continuum_subtracted = spectrum_obs\n",
    "\n",
    "        template_rest_wave_um = 0.6563 * u.micron\n",
    "        template_wave_um = np.linspace(template_rest_wave_um.value - 0.1, template_rest_wave_um.value + 0.1, 200) * u.micron\n",
    "        template_flux_val = np.exp(-0.5 * ((template_wave_um - template_rest_wave_um) / (0.002 * u.micron))**2)\n",
    "        template_spectrum = Spectrum1D(flux=template_flux_val * u.dimensionless_unscaled, spectral_axis=template_wave_um)\n",
    "        z_range = np.linspace(0, 4, 100)\n",
    "        xcor_values = []\n",
    "        print(\"Performing manual cross-correlation (demo)...\")\n",
    "        for z_test in z_range:\n",
    "            shifted_template_wave = template_spectrum.spectral_axis * (1 + z_test)\n",
    "            try:\n",
    "                obs_wave_for_interp = spectrum_continuum_subtracted.spectral_axis.to(shifted_template_wave.unit)\n",
    "                obs_flux_for_interp = spectrum_continuum_subtracted.flux\n",
    "                min_obs_wave = obs_wave_for_interp.value.min()\n",
    "                max_obs_wave = obs_wave_for_interp.value.max()\n",
    "                valid_template_indices = (shifted_template_wave.value >= min_obs_wave) & \\\n",
    "                                          (shifted_template_wave.value <= max_obs_wave)\n",
    "                if not np.any(valid_template_indices):\n",
    "                    xcor_values.append(0); continue\n",
    "                interp_obs_flux_on_template_grid = np.interp(\n",
    "                    shifted_template_wave.value[valid_template_indices],\n",
    "                    obs_wave_for_interp.value,\n",
    "                    obs_flux_for_interp.value \n",
    "                )\n",
    "                norm_template_flux = template_spectrum.flux[valid_template_indices].value\n",
    "                if np.linalg.norm(norm_template_flux) > 0: norm_template_flux /= np.linalg.norm(norm_template_flux)\n",
    "                norm_interp_obs_flux = interp_obs_flux_on_template_grid\n",
    "                if np.linalg.norm(norm_interp_obs_flux) > 0: norm_interp_obs_flux /= np.linalg.norm(norm_interp_obs_flux)\n",
    "                if len(norm_template_flux) == len(norm_interp_obs_flux) and len(norm_template_flux) > 0:\n",
    "                     current_xcor = np.dot(norm_template_flux, norm_interp_obs_flux)\n",
    "                     xcor_values.append(current_xcor)\n",
    "                else: xcor_values.append(0)\n",
    "            except Exception as interp_e:\n",
    "                xcor_values.append(0)\n",
    "        if xcor_values:\n",
    "            best_z_index = np.argmax(xcor_values)\n",
    "            estimated_redshift = z_range[best_z_index]\n",
    "            print(f\"Estimated redshift from cross-correlation: z = {estimated_redshift:.3f}\")\n",
    "            with quantity_support():\n",
    "                plt.figure(figsize=(8,4))\n",
    "                plt.plot(z_range, xcor_values)\n",
    "                plt.xlabel(\"Redshift (z)\")\n",
    "                plt.ylabel(\"Cross-correlation Value (normalized)\")\n",
    "                plt.title(f\"Redshift Cross-correlation for {name} (Template: H-alpha like)\")\n",
    "                plt.axvline(estimated_redshift, color='r', linestyle='--', label=f'Best z = {estimated_redshift:.3f}')\n",
    "                if name.startswith(\"placeholder_z_sim_\"):\n",
    "                    true_z = float(name.split(\"_\")[-1])\n",
    "                    plt.axvline(true_z, color='g', linestyle=':', label=f'Simulated z = {true_z:.3f}')\n",
    "                plt.legend(); plt.grid(True)\n",
    "                plt.savefig(os.path.join(redshift_fitting_output_dir, f\"{name}_xcor_redshift.png\"))\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "        else:\n",
    "            print(\"Cross-correlation did not produce results.\")\n",
    "        print(\"Emission line fitting demonstration is commented out but shows the principle.\")\n",
    "\n",
    "print(\"\\n--- Spectroscopic Redshift Fitting Demonstration Complete ---\")\n",
    "if not spectra_to_fit:\n",
    "    print(\"No spectra were processed. The cell demonstrated creating a placeholder.\")\n",
    "else:\n",
    "    print(f\"Attempted redshift fitting for {len(spectra_to_fit)} spectra.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
